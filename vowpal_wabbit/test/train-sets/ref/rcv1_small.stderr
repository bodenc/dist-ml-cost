using l2 regularization = 1
enabling BFGS based optimization **without** curvature calculation
Num weight bits = 18
learning rate = 0.5
initial_t = 0
power_t = 0.5
decay_learning_rate = 1
m = 7
Allocated 18M for weights and mem
## avg. loss 	der. mag. 	d. m. cond.	 wolfe1    	wolfe2    	mix fraction	curvature 	dir. magnitude	step size
creating cache_file = train-sets/rcv1_small.dat.cache
Reading datafile = train-sets/rcv1_small.dat
num sources = 1
 1 0.69315   	0.00266   	0.87764   	          	          	          	2.24708   	776.93237 	0.39057
 3 0.51357   	0.00493   	4.93046   	 0.523903  	0.088793  	          	          	76.25748  	1.00000
 4 0.65936   	0.04915   	49.15202  	 -0.910622 	-2.480116 	          	          	(revise x 0.5)	0.50000
 5 0.51658   	0.00876   	8.76105   	 -0.037665 	-0.999616 	          	          	(revise x 0.5)	0.25000
 6 0.49499   	0.00028   	0.28254   	 0.463963  	-0.056952 	          	          	0.51262   	1.00000
 7 0.49354   	0.00006   	0.05641   	 0.619867  	0.244153  	          	          	0.08545   	1.00000
 8 0.49287   	0.00005   	0.05434   	 0.870687  	0.741762  	          	          	0.91640   	1.00000
 9 0.48978   	0.00014   	0.13750   	 0.772760  	0.546930  	          	          	2.01229   	1.00000
10 0.48472   	0.00027   	0.27437   	 0.750340  	0.501776  	          	          	3.21399   	1.00000
11 0.47920   	0.00017   	0.16867   	 0.671044  	0.340515  	          	          	1.40135   	1.00000
12 0.47707   	0.00001   	0.00760   	 0.593376  	0.181239  	          	          	0.09201   	1.00000
13 0.47691   	0.00000   	0.00168   	 0.593278  	0.185019  	          	          	0.00955   	1.00000

finished run
number of examples = 13000
weighted example sum = 13000
weighted label sum = -1066
average loss = 0.4417
best constant = -0.164369
best constant's loss = 0.689781
total feature number = 1023607
